{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoEBM: Complete Training & Inference Pipeline on H100\n",
    "\n",
    "This notebook provides a full-scale workflow for Energy-Based Models (EBM) training, inference, and visualization.\n",
    "\n",
    "## Sections:\n",
    "1. Setup & Environment\n",
    "2. Clone Repository & Install Dependencies\n",
    "3. Data Preparation\n",
    "4. Model Training\n",
    "   - Baseline (no thinking)\n",
    "   - With iterative refinement (thinking)\n",
    "5. Inference & Sampling\n",
    "   - Greedy generation\n",
    "   - Thinking + nucleus sampling\n",
    "6. Visualization & Analysis\n",
    "7. Metrics Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Environment\n",
    "Verify GPU availability and environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    \n",
    "    # H100 check (compute capability 9.0)\n",
    "    if torch.cuda.get_device_capability(0)[0] >= 9:\n",
    "        print(\"✓ H100 detected!\")\n",
    "    \n",
    "    # Memory info\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total GPU memory: {mem_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/sdan/nanoEBM.git\n",
    "%cd nanoEBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we're in the right directory\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies using pip (since we're in a notebook environment)\n",
    "# If using uv, you can uncomment the uv commands below\n",
    "\n",
    "# Option 1: Using pip\n",
    "!pip install torch numpy chz tiktoken pyarrow matplotlib wandb\n",
    "\n",
    "# Option 2: Using uv (if available)\n",
    "# !pip install uv\n",
    "# !uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nanoebm package\n",
    "from nanoebm.config import Config, ModelConfig, DataConfig, TrainConfig\n",
    "from nanoebm.model import EBTLanguageModel\n",
    "from nanoebm.data import get_loader\n",
    "from nanoebm.utils import Logger\n",
    "\n",
    "print(\"✓ nanoEBM imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation\n",
    "\n",
    "### Option A: Use built-in Shakespeare dataset (character-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if shakespeare.txt exists\n",
    "shakespeare_path = Path(\"shakespeare.txt\")\n",
    "if shakespeare_path.exists():\n",
    "    print(f\"✓ Shakespeare dataset found\")\n",
    "    print(f\"  Size: {shakespeare_path.stat().st_size / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Preview first 500 characters\n",
    "    with open(shakespeare_path, 'r') as f:\n",
    "        preview = f.read(500)\n",
    "    print(f\"\\nPreview:\\n{preview}...\")\n",
    "else:\n",
    "    print(\"✗ Shakespeare dataset not found. Downloading...\")\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Prepare BPE dataset (for production)\n",
    "\n",
    "For larger-scale training, you can use BPE tokenization with parquet datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download and prepare a larger dataset (optional)\n",
    "# Uncomment if you want to use a larger dataset\n",
    "\n",
    "# DATA_DIR = Path(\"data/fineweb_sample\")\n",
    "# DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Example: Download sample from HuggingFace datasets\n",
    "# # You'll need to prepare parquet files or use a text file\n",
    "# print(\"For BPE training, ensure you have parquet files or a large text corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training\n",
    "\n",
    "### 4.1 Training Configuration\n",
    "\n",
    "We'll set up configurations for different training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Configuration (No thinking, character-level)\n",
    "baseline_cfg = Config(\n",
    "    model=ModelConfig(\n",
    "        vocab_size=None,  # Auto-detected from dataset\n",
    "        block_size=256,\n",
    "        n_layer=6,\n",
    "        n_head=6,\n",
    "        n_embd=384,\n",
    "        dropout=0.1,\n",
    "        mcmc_num_steps=0,  # No iterative refinement\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset=\"shakespeare\",\n",
    "        data_path=\"shakespeare.txt\",\n",
    "        tokenizer=\"char\",\n",
    "        block_size=256,\n",
    "        batch_size=64,\n",
    "    ),\n",
    "    train=TrainConfig(\n",
    "        max_steps=5000,\n",
    "        learning_rate=3e-4,\n",
    "        eval_interval=500,\n",
    "        log_interval=10,\n",
    "        save_interval=1000,\n",
    "        compile_model=True,  # Use torch.compile for speed\n",
    "    ),\n",
    "    device=\"cuda\",\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline config created\")\n",
    "print(f\"  Training steps: {baseline_cfg.train.max_steps}\")\n",
    "print(f\"  Batch size: {baseline_cfg.data.batch_size}\")\n",
    "print(f\"  Model layers: {baseline_cfg.model.n_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Configuration (With thinking, character-level)\n",
    "thinking_cfg = Config(\n",
    "    model=ModelConfig(\n",
    "        vocab_size=None,  # Auto-detected\n",
    "        block_size=256,\n",
    "        n_layer=6,\n",
    "        n_head=6,\n",
    "        n_embd=384,\n",
    "        dropout=0.1,\n",
    "        # Energy-based thinking parameters\n",
    "        mcmc_num_steps=2,  # 2 refinement steps\n",
    "        mcmc_step_size=1.0,\n",
    "        mcmc_step_size_learnable=True,  # Learn alpha\n",
    "        entropy_reg_tau=0.5,\n",
    "        truncate_mcmc=True,  # Don't backprop through all steps\n",
    "        no_mcmc_detach=False,\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset=\"shakespeare\",\n",
    "        data_path=\"shakespeare.txt\",\n",
    "        tokenizer=\"char\",\n",
    "        block_size=256,\n",
    "        batch_size=64,\n",
    "    ),\n",
    "    train=TrainConfig(\n",
    "        max_steps=10000,\n",
    "        learning_rate=3e-4,\n",
    "        eval_interval=500,\n",
    "        log_interval=10,\n",
    "        save_interval=1000,\n",
    "        compile_model=True,\n",
    "    ),\n",
    "    device=\"cuda\",\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "print(\"✓ Thinking config created\")\n",
    "print(f\"  MCMC steps: {thinking_cfg.model.mcmc_num_steps}\")\n",
    "print(f\"  Learnable step size: {thinking_cfg.model.mcmc_step_size_learnable}\")\n",
    "print(f\"  Training steps: {thinking_cfg.train.max_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Baseline Model\n",
    "\n",
    "Train a standard EBM without iterative refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline using subprocess (cleaner output)\n",
    "# This trains for 5000 steps with character-level tokenization\n",
    "\n",
    "!python train.py \\\n",
    "    data.data_path=shakespeare.txt \\\n",
    "    data.tokenizer=char \\\n",
    "    model.mcmc_num_steps=0 \\\n",
    "    train.max_steps=5000 \\\n",
    "    train.log_interval=10 \\\n",
    "    train.eval_interval=500 \\\n",
    "    train.compile_model=true \\\n",
    "    out_dir=out_ebt/baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train Model with Thinking\n",
    "\n",
    "Train with iterative refinement (2 MCMC steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with thinking for 10000 steps\n",
    "!python train.py \\\n",
    "    data.data_path=shakespeare.txt \\\n",
    "    data.tokenizer=char \\\n",
    "    model.mcmc_num_steps=2 \\\n",
    "    model.mcmc_step_size_learnable=true \\\n",
    "    model.truncate_mcmc=true \\\n",
    "    train.max_steps=10000 \\\n",
    "    train.log_interval=10 \\\n",
    "    train.eval_interval=500 \\\n",
    "    train.compile_model=true \\\n",
    "    out_dir=out_ebt/thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Optional: Train BPE Model (for production)\n",
    "\n",
    "For larger-scale training with BPE tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train with BPE tokenization\n",
    "# Requires a larger dataset (parquet or large text file)\n",
    "\n",
    "# !python train.py \\\n",
    "#     data.tokenizer=gpt2 \\\n",
    "#     data.data_path=/path/to/large/dataset \\\n",
    "#     model.vocab_size=50304 \\\n",
    "#     model.mcmc_num_steps=2 \\\n",
    "#     train.max_steps=50000 \\\n",
    "#     train.batch_size=32 \\\n",
    "#     out_dir=out_ebt/bpe_thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Inference & Sampling\n",
    "\n",
    "### 5.1 Greedy Generation (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using greedy decoding (no thinking)\n",
    "# Uses the baseline checkpoint\n",
    "\n",
    "!python sample.py \\\n",
    "    checkpoint=out_ebt/baseline/final.pt \\\n",
    "    prompt=\"ROMEO:\" \\\n",
    "    max_new_tokens=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Generation with Thinking (Iterative Refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with thinking (4 refinement steps per token)\n",
    "# Uses nucleus sampling for diversity\n",
    "\n",
    "!python sample.py \\\n",
    "    checkpoint=out_ebt/thinking/final.pt \\\n",
    "    prompt=\"HAMLET:\" \\\n",
    "    max_new_tokens=300 \\\n",
    "    use_thinking=true \\\n",
    "    think_steps=4 \\\n",
    "    topk=64 \\\n",
    "    sample=true \\\n",
    "    sample_temp=1.0 \\\n",
    "    sample_top_p=0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Interactive Sampling (Python API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for interactive generation\n",
    "import torch\n",
    "from nanoebm.model import EBTLanguageModel\n",
    "from nanoebm.config import Config\n",
    "import json\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = \"out_ebt/thinking/final.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "\n",
    "# Load config\n",
    "config = Config(**checkpoint[\"config\"])\n",
    "\n",
    "# Create model\n",
    "model = EBTLanguageModel(\n",
    "    config.model.to_gpt_config(),\n",
    "    config.model.to_ebt_config()\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded from {ckpt_path}\")\n",
    "print(f\"  Vocab size: {config.model.vocab_size}\")\n",
    "print(f\"  Block size: {config.model.block_size}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation function\n",
    "from nanoebm.data import CharDataset, BPEDataset\n",
    "import tiktoken\n",
    "\n",
    "def generate_text(prompt, max_tokens=200, temperature=1.0, top_p=0.95):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    if config.data.tokenizer == \"char\":\n",
    "        # Load vocab from dataset\n",
    "        with open(config.data.data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        chars = sorted(list(set(text)))\n",
    "        stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(config.data.bpe_encoding)\n",
    "        encode = enc.encode\n",
    "        decode = enc.decode\n",
    "    \n",
    "    # Encode prompt\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        # Use greedy generation for simplicity\n",
    "        for _ in range(max_tokens):\n",
    "            # Crop context to block_size\n",
    "            context_crop = context[:, -config.model.block_size:]\n",
    "            \n",
    "            # Get energies\n",
    "            h = model.backbone(context_crop)\n",
    "            energies = model.energy(h[:, -1:, :])  # Last position\n",
    "            logits = -energies.squeeze(1)  # (B, V)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Sample\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to context\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "    \n",
    "    # Decode\n",
    "    generated = context[0].tolist()\n",
    "    text = decode(generated)\n",
    "    return text\n",
    "\n",
    "# Test generation\n",
    "prompt = \"To be or not to be\"\n",
    "result = generate_text(prompt, max_tokens=100, temperature=0.8)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(result)\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization & Analysis\n",
    "\n",
    "### 6.1 Energy Landscape Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-K lowest energy tokens\n",
    "!python viz.py \\\n",
    "    --checkpoint=out_ebt/thinking/final.pt \\\n",
    "    --prompt=\"ROMEO:\" \\\n",
    "    --mode=topk \\\n",
    "    --topk=20\n",
    "\n",
    "# Display the generated plot\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if Path(\"energies_topk.png\").exists():\n",
    "    display(Image(filename=\"energies_topk.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Energy vs Cross-Entropy Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between energy and CE loss\n",
    "!python viz.py \\\n",
    "    --checkpoint=out_ebt/thinking/final.pt \\\n",
    "    --mode=correlation \\\n",
    "    --batches=4\n",
    "\n",
    "if Path(\"energy_vs_ce.png\").exists():\n",
    "    display(Image(filename=\"energy_vs_ce.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Token Trajectory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize logit trajectories during refinement\n",
    "!python viz.py \\\n",
    "    --checkpoint=out_ebt/thinking/final.pt \\\n",
    "    --prompt=\"HAMLET:\" \\\n",
    "    --mode=trajectories \\\n",
    "    --steps=8\n",
    "\n",
    "if Path(\"token_logit_trajectories.png\").exists():\n",
    "    display(Image(filename=\"token_logit_trajectories.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 3D Energy Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D energy landscape visualization\n",
    "!python viz.py \\\n",
    "    --checkpoint=out_ebt/thinking/final.pt \\\n",
    "    --prompt=\"The\" \\\n",
    "    --mode=surface3d \\\n",
    "    --surface_grid=60 \\\n",
    "    --surface_span=6.0\n",
    "\n",
    "if Path(\"energy_surface3d.png\").exists():\n",
    "    display(Image(filename=\"energy_surface3d.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Metrics Inspection\n",
    "\n",
    "### 7.1 Load and Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_metrics(metrics_path):\n",
    "    \"\"\"Load metrics from JSONL file.\"\"\"\n",
    "    metrics = []\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        for line in f:\n",
    "            metrics.append(json.loads(line))\n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics, title=\"Training Metrics\"):\n",
    "    \"\"\"Plot training metrics.\"\"\"\n",
    "    steps = [m['step'] for m in metrics]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(steps, [m['loss'] for m in metrics], label='Train Loss')\n",
    "    if 'val_loss' in metrics[0]:\n",
    "        val_steps = [m['step'] for m in metrics if 'val_loss' in m]\n",
    "        val_losses = [m['val_loss'] for m in metrics if 'val_loss' in m]\n",
    "        axes[0, 0].plot(val_steps, val_losses, label='Val Loss', marker='o')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss over Time')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perplexity\n",
    "    axes[0, 1].plot(steps, [m['perplexity'] for m in metrics])\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    axes[0, 1].set_ylabel('Perplexity')\n",
    "    axes[0, 1].set_title('Perplexity over Time')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Energy gap (if available)\n",
    "    if 'energy_gap' in metrics[0]:\n",
    "        axes[1, 0].plot(steps, [m.get('energy_gap', 0) for m in metrics])\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('Energy Gap')\n",
    "        axes[1, 0].set_title('Energy Gap (Initial - Final)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    axes[1, 1].plot(steps, [m['lr'] for m in metrics])\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Load and plot baseline metrics\n",
    "baseline_metrics_path = Path(\"out_ebt/baseline\") / \"metrics.jsonl\"\n",
    "if baseline_metrics_path.exists():\n",
    "    baseline_metrics = load_metrics(baseline_metrics_path)\n",
    "    fig = plot_metrics(baseline_metrics, \"Baseline Model Training\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Baseline metrics not found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot thinking model metrics\n",
    "thinking_metrics_path = Path(\"out_ebt/thinking\") / \"metrics.jsonl\"\n",
    "if thinking_metrics_path.exists():\n",
    "    thinking_metrics = load_metrics(thinking_metrics_path)\n",
    "    fig = plot_metrics(thinking_metrics, \"Thinking Model Training\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Thinking model metrics not found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compare Baseline vs Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models\n",
    "if baseline_metrics_path.exists() and thinking_metrics_path.exists():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss comparison\n",
    "    baseline_steps = [m['step'] for m in baseline_metrics]\n",
    "    baseline_losses = [m['loss'] for m in baseline_metrics]\n",
    "    thinking_steps = [m['step'] for m in thinking_metrics]\n",
    "    thinking_losses = [m['loss'] for m in thinking_metrics]\n",
    "    \n",
    "    axes[0].plot(baseline_steps, baseline_losses, label='Baseline', alpha=0.7)\n",
    "    axes[0].plot(thinking_steps, thinking_losses, label='Thinking', alpha=0.7)\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perplexity comparison\n",
    "    baseline_ppl = [m['perplexity'] for m in baseline_metrics]\n",
    "    thinking_ppl = [m['perplexity'] for m in thinking_metrics]\n",
    "    \n",
    "    axes[1].plot(baseline_steps, baseline_ppl, label='Baseline', alpha=0.7)\n",
    "    axes[1].plot(thinking_steps, thinking_ppl, label='Thinking', alpha=0.7)\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Perplexity')\n",
    "    axes[1].set_title('Perplexity Comparison')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL METRICS COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBaseline (Step {baseline_metrics[-1]['step']}):\")\n",
    "    print(f\"  Loss: {baseline_metrics[-1]['loss']:.4f}\")\n",
    "    print(f\"  Perplexity: {baseline_metrics[-1]['perplexity']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nThinking (Step {thinking_metrics[-1]['step']}):\")\n",
    "    print(f\"  Loss: {thinking_metrics[-1]['loss']:.4f}\")\n",
    "    print(f\"  Perplexity: {thinking_metrics[-1]['perplexity']:.4f}\")\n",
    "    if 'energy_gap' in thinking_metrics[-1]:\n",
    "        print(f\"  Energy Gap: {thinking_metrics[-1]['energy_gap']:.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Checkpoint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze checkpoint contents\n",
    "def analyze_checkpoint(ckpt_path):\n",
    "    \"\"\"Analyze a model checkpoint.\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checkpoint: {ckpt_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nTraining step: {ckpt.get('step', 'N/A')}\")\n",
    "    \n",
    "    # Model info\n",
    "    if 'model' in ckpt:\n",
    "        state_dict = ckpt['model']\n",
    "        total_params = sum(p.numel() for p in state_dict.values())\n",
    "        print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "        \n",
    "        # Parameter breakdown\n",
    "        print(\"\\nParameter breakdown:\")\n",
    "        for name, param in state_dict.items():\n",
    "            if len(param.shape) > 0:  # Skip scalars\n",
    "                print(f\"  {name:50s} {str(tuple(param.shape)):20s} {param.numel():>10,}\")\n",
    "    \n",
    "    # Config\n",
    "    if 'config' in ckpt:\n",
    "        cfg = ckpt['config']\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Vocab size: {cfg['model']['vocab_size']}\")\n",
    "        print(f\"  Block size: {cfg['model']['block_size']}\")\n",
    "        print(f\"  Layers: {cfg['model']['n_layer']}\")\n",
    "        print(f\"  Heads: {cfg['model']['n_head']}\")\n",
    "        print(f\"  Embedding dim: {cfg['model']['n_embd']}\")\n",
    "        print(f\"  MCMC steps: {cfg['model']['mcmc_num_steps']}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Analyze thinking model checkpoint\n",
    "thinking_ckpt = \"out_ebt/thinking/final.pt\"\n",
    "if Path(thinking_ckpt).exists():\n",
    "    analyze_checkpoint(thinking_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Model for Production\n",
    "\n",
    "Prepare model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX (optional)\n",
    "# Note: This requires additional setup and may not work for all configurations\n",
    "\n",
    "def export_to_onnx(checkpoint_path, output_path=\"model.onnx\"):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "    config = Config(**ckpt[\"config\"])\n",
    "    \n",
    "    model = EBTLanguageModel(\n",
    "        config.model.to_gpt_config(),\n",
    "        config.model.to_ebt_config()\n",
    "    )\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randint(0, config.model.vocab_size, (1, config.model.block_size), device=\"cuda\")\n",
    "    \n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch', 1: 'sequence'}}\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Model exported to {output_path}\")\n",
    "\n",
    "# Uncomment to export\n",
    "# export_to_onnx(\"out_ebt/thinking/final.pt\", \"nanoebm_thinking.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Next Steps & Tips\n",
    "\n",
    "### Performance Optimization\n",
    "- Use `torch.compile()` for 2-3x speedup on H100\n",
    "- Enable `bf16` training for faster computation\n",
    "- Increase batch size to maximize GPU utilization\n",
    "- Use gradient accumulation for larger effective batch sizes\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- **MCMC steps**: Try 2-4 steps for balance of performance/speed\n",
    "- **Learning rate**: 3e-4 works well for most cases\n",
    "- **Entropy tau**: 0.5-1.0 for exploration/exploitation balance\n",
    "- **Step size**: Enable learnable alpha for automatic tuning\n",
    "\n",
    "### Scaling Up\n",
    "- Use BPE tokenization for production models\n",
    "- Train on larger datasets (FineWeb, OpenWebText, etc.)\n",
    "- Increase model size: 12 layers, 768 dim → ~100M params\n",
    "- Multi-GPU training with DDP (Distributed Data Parallel)\n",
    "\n",
    "### Monitoring\n",
    "- Watch energy gap metric during training\n",
    "- Monitor perplexity on validation set\n",
    "- Use W&B for cloud-based experiment tracking\n",
    "- Compare thinking vs baseline periodically\n",
    "\n",
    "### Inference Optimization\n",
    "- Use KV cache for faster autoregressive generation\n",
    "- Reduce think_steps at inference for speed\n",
    "- Experiment with top-k/top-p sampling parameters\n",
    "- Consider quantization (int8/int4) for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
